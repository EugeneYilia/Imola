import unicodedata
import json
import uuid
import string
from fastapi import FastAPI, Query
from contextlib import asynccontextmanager
from pydantic import BaseModel
from typing import Optional
import requests
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

import logging
from colorlog import ColoredFormatter

import SystemConfig

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ & Qdrant å®¢æˆ·ç«¯
embedding_model = SentenceTransformer(SystemConfig.model_name)
qdrant_client = QdrantClient(host=SystemConfig.qdrant_host, port=SystemConfig.qdrant_port)

# === FastAPI åˆå§‹åŒ– ===

@asynccontextmanager
async def lifespan(app: FastAPI):
    # âœ… å¯åŠ¨å‰æ‰§è¡Œ
    logger.info("FastAPI å¯åŠ¨ï¼šis_use_gpu: %s", SystemConfig.is_use_gpu)
    logger.info("FastAPI å¯åŠ¨ï¼šis_dev_mode: %s", SystemConfig.is_dev_mode)

    yield  # ğŸŸ¢ åº”ç”¨è¿è¡Œä¸­

    # âœ… å…³é—­å‰æ‰§è¡Œï¼ˆå¯é€‰ï¼‰
    logger.info("FastAPI å³å°†å…³é—­")
app = FastAPI(title="RAG å·¥ç¨‹åŠ©æ‰‹ API", description="ç»“åˆ Qdrant + Ollama çš„æ™ºèƒ½é—®ç­”æœåŠ¡", lifespan=lifespan)
# æŒ‚è½½ static ç›®å½•ï¼ˆå‡è®¾ä½ å°† HTML æ”¾åœ¨å½“å‰ç›®å½•ï¼‰
app.mount("/static", StaticFiles(directory="."), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æˆ–æŒ‡å®šå‰ç«¯æ¥æº ['http://localhost:3000']
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è®¾ç½®è®¿é—®æ ¹è·¯å¾„æ—¶è¿”å› index.html
@app.get("/")
def serve_home():
    return FileResponse("index.html")

# === è¯·æ±‚ä½“æ¨¡å‹ ===
class QuestionRequest(BaseModel):
    question: str
    model: Optional[str] = "mistral:7b-instruct"
    top_k: Optional[int] = SystemConfig.default_top_k

# === collection æ˜ å°„é€»è¾‘ ===
def resolve_collection(user_question: str):
    mapping = {
        "å¤§æ¨¡å‹éœ€æ±‚": "enterprise_knowledge",
        "å‰å®‰": "enterprise_knowledge",
        "é™æ€æ•°æ®": "enterprise_knowledge",
        "ç»ˆå­”åæ–œ": "terminal_deviation_data",
        "é‚»å­”é—´è·": "neighbor_spacing_data",
        "é’»å­”åæ–œ": "hole_deviation_data",
    }
    for keyword, collection in mapping.items():
        if keyword in user_question:
            return collection
    return "enterprise_knowledge"

# === Qdrant æ£€ç´¢ ===
def retrieve_from_qdrant(query, collection_name="enterprise_knowledge", top_k=SystemConfig.default_top_k):
    query_vec = embedding_model.encode([query], normalize_embeddings=True)[0]
    search_result = qdrant_client.search(
        collection_name=collection_name,
        query_vector=query_vec,
        limit=top_k
    )
    return [hit.payload for hit in search_result]

# === æ„é€  Prompt å¹¶è°ƒç”¨ Ollama ===
def ask_with_context(user_question, collection_name, model="mistral:7b-instruct", top_k=SystemConfig.default_top_k):
    results = retrieve_from_qdrant(user_question, collection_name, top_k=top_k)
    logger.info(f"qdrant results: {results}")
    if not results:
        context = "ï¼ˆæœªèƒ½ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼‰"
    else:
        context = "\n".join([
            f"ã€{r.get('section', r.get('source', 'æœªæ³¨æ˜æ¥æº'))}ã€‘\n{r.get('content', r.get('text', str(r)))}"
            for r in results
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ç¨‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\nå·²çŸ¥èµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{user_question}\nè¯·ç”¨ä¸“ä¸šã€ç®€æ˜çš„æ–¹å¼å›ç­”ã€‚"""

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    logger.info(f"Request llm server payload: {payload}")
    response = requests.post(SystemConfig.ollama_url, json=payload)
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"è¯·æ±‚å¤±è´¥: {response.status_code}\n{response.text}"

def is_punctuation(char):

    if not isinstance(char, str) or len(char) != 1:
        # logger.info(f"ä¼ å…¥çš„ charï¼š{repr(char)}ï¼Œé•¿åº¦ï¼š{len(char)}   not isinstance(char, str) or len(char) != 1: {False}")
        return False

    # æ”¾è¡Œè‹±æ–‡æ ‡ç‚¹ç¬¦å·
    if char in string.punctuation and ord(char) < 128:
        # logger.info(f"ä¼ å…¥çš„ charï¼š{repr(char)}ï¼Œé•¿åº¦ï¼š{len(char)}   char in string.punctuation and ord(char) < 128: {False}")
        return False

    if char in ["ï¼ˆ", "ï¼‰", "(", ")", "{", "}", "[", "]"]:
        return False

    # åªç®—ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
    # logger.info(f"ä¼ å…¥çš„ charï¼š{repr(char)}ï¼Œé•¿åº¦ï¼š{len(char)}   {unicodedata.category(char).startswith('P')}")
    return unicodedata.category(char).startswith('P')

# === æµå¼è¯·æ±‚ Ollamaï¼ˆæ”¯æŒé€æ®µè¿”å›ï¼‰===
def ask_with_context_stream(user_question, collection_name, model="mistral:7b-instruct", top_k=SystemConfig.default_top_k):
    results = retrieve_from_qdrant(user_question, collection_name, top_k=top_k)
    logger.info(f"qdrant results: {results}")

    if not results:
        context = "ï¼ˆæœªèƒ½ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼‰"
    else:
        context = "\n".join([
            f"ã€{r.get('section', r.get('source', 'æœªæ³¨æ˜æ¥æº'))}ã€‘\n{r.get('content', r.get('text', str(r)))}"
            for r in results
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ç¨‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸­é—´å°½å¯èƒ½å¤šçš„ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†å‰²ï¼Œç¡®ä¿æ¯ä¸ªåˆ†å‰²åçš„çŸ­å¥å­—æ•°éƒ½å·®ä¸å¤šï¼š\n\n
    å·²çŸ¥èµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{user_question}\nè¯·ç”¨ä¸“ä¸šã€ç®€æ˜çš„æ–¹å¼å›ç­”ã€‚"""

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True  # å¼€å¯æµæ¨¡å¼
    }

    logger.info(f"Request llm server payload: {payload}")

    try:
        with requests.post(SystemConfig.ollama_url, json=payload, stream=True) as response:
            if response.status_code != 200:
                yield f"[é”™è¯¯] è¯·æ±‚å¤±è´¥: {response.status_code}"
                return

            response_buffer = ""
            is_ready_to_send = False
            # ä¸€è¡Œä¸€è¡Œè¯»å–å“åº”å†…å®¹
            for line in response.iter_lines():
                if line:
                    try:
                        # Ollama çš„æ¯ä¸€è¡Œæ˜¯ JSONï¼Œå¦‚ {"response": "éƒ¨åˆ†å›ç­”", "done": false}
                        data = json.loads(line.decode("utf-8"))
                        # logger.info(f"Request llm server response: {data}")
                        response = data["response"].strip()
                        if response != "" :
                            if is_punctuation(response):
                                response_buffer += response + "\n"
                                is_ready_to_send = True
                            else:
                                if is_ready_to_send:
                                    is_ready_to_send = False
                                    chunked_response = response_buffer
                                    response_buffer = response
                                    logger.info(f"llm server response: {chunked_response}")
                                    yield chunked_response
                                else:
                                    response_buffer += response
                        if data.get("done"):
                            chunked_response = response_buffer.removesuffix("\n") + "[Heil Hitler!]" + "\n"
                            logger.info(f"llm server response: {chunked_response}")
                            yield chunked_response
                            break
                    except Exception as e:
                        logger.error(f"è§£æå¤±è´¥: {line} -> {e}")
                        raise e
    except Exception as e:
        logger.error(f"[é”™è¯¯] è¿æ¥å¼‚å¸¸: {e}")
        raise e

# === API è·¯ç”± ===
@app.post("/ask")
def rag_qa(req: QuestionRequest):
    request_id = str(uuid.uuid4())
    logger.info(f"[ASK æ¥æ”¶] {request_id}  é—®é¢˜: {req}")

    collection = resolve_collection(req.question)
    return StreamingResponse(
        ask_with_context_stream(
            req.question,
            collection,
            model=req.model,
            top_k=req.top_k),
        media_type="text/plain",
        status_code=200,
        headers={"X-Accel-Buffering": "no"}
    )




if __name__ == "__main__":
    import uvicorn

    if SystemConfig.is_dev_mode:
        uvicorn.run(
            "main:app",
            host="0.0.0.0",
            port=8000,
            reload=SystemConfig.is_dev_mode,
            log_config="log_config.yml"
        )
    else:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            reload=SystemConfig.is_dev_mode,
            log_config="log_config.yml"
        )