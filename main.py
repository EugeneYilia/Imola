import unicodedata
import json
import uuid
import string
from fastapi import FastAPI, Query
from contextlib import asynccontextmanager
from pydantic import BaseModel
from typing import Optional
import requests
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

import logging
from colorlog import ColoredFormatter

import SystemConfig

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ & Qdrant å®¢æˆ·ç«¯
embedding_model = SentenceTransformer(SystemConfig.model_name)
qdrant_client = QdrantClient(host=SystemConfig.qdrant_host, port=SystemConfig.qdrant_port)

# === FastAPI åˆå§‹åŒ– ===

@asynccontextmanager
async def lifespan(app: FastAPI):
    # âœ… å¯åŠ¨å‰æ‰§è¡Œ
    logger.info("FastAPI å¯åŠ¨ï¼šis_use_gpu: %s", SystemConfig.is_use_gpu)
    logger.info("FastAPI å¯åŠ¨ï¼šis_dev_mode: %s", SystemConfig.is_dev_mode)

    yield  # ğŸŸ¢ åº”ç”¨è¿è¡Œä¸­

    # âœ… å…³é—­å‰æ‰§è¡Œï¼ˆå¯é€‰ï¼‰
    logger.info("FastAPI å³å°†å…³é—­")
app = FastAPI(title="RAG å·¥ç¨‹åŠ©æ‰‹ API", description="ç»“åˆ Qdrant + Ollama çš„æ™ºèƒ½é—®ç­”æœåŠ¡", lifespan=lifespan)
# æŒ‚è½½ static ç›®å½•ï¼ˆå‡è®¾ä½ å°† HTML æ”¾åœ¨å½“å‰ç›®å½•ï¼‰
app.mount("/static", StaticFiles(directory="static"), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æˆ–æŒ‡å®šå‰ç«¯æ¥æº ['http://localhost:3000']
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è®¾ç½®è®¿é—®æ ¹è·¯å¾„æ—¶è¿”å› index.html
@app.get("/")
def serve_home():
    return FileResponse("index.html")

# === è¯·æ±‚ä½“æ¨¡å‹ ===
class QuestionRequest(BaseModel):
    question: str
    model: Optional[str] = "mistral:7b-instruct"
    top_k: Optional[int] = SystemConfig.default_top_k

# === collection æ˜ å°„é€»è¾‘ ===
def resolve_collection(user_question: str):
    mapping = {
        "å¤§æ¨¡å‹éœ€æ±‚": "enterprise_knowledge",
        "å‰å®‰": "enterprise_knowledge",
        "é™æ€æ•°æ®": "enterprise_knowledge",
        "ç»ˆå­”åæ–œ": "terminal_deviation_data",
        "é‚»å­”é—´è·": "neighbor_spacing_data",
        "é’»å­”åæ–œ": "hole_deviation_data",
    }
    for keyword, collection in mapping.items():
        if keyword in user_question:
            return collection
    return "enterprise_knowledge"

# === Qdrant æ£€ç´¢ ===
def retrieve_from_qdrant(query, collection_name="enterprise_knowledge", top_k=SystemConfig.default_top_k):
    query_vec = embedding_model.encode([query], normalize_embeddings=True)[0]
    search_result = qdrant_client.search(
        collection_name=collection_name,
        query_vector=query_vec,
        limit=top_k
    )
    return [hit.payload for hit in search_result]

# === æ„é€  Prompt å¹¶è°ƒç”¨ Ollama ===
def ask_with_context(user_question, collection_name, model="mistral:7b-instruct", top_k=SystemConfig.default_top_k):
    results = retrieve_from_qdrant(user_question, collection_name, top_k=top_k)
    logger.info(f"qdrant results: {results}")
    if not results:
        context = "ï¼ˆæœªèƒ½ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼‰"
    else:
        context = "\n".join([
            f"ã€{r.get('section', r.get('source', 'æœªæ³¨æ˜æ¥æº'))}ã€‘\n{r.get('content', r.get('text', str(r)))}"
            for r in results
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ç¨‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\nå·²çŸ¥èµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{user_question}\nè¯·ç”¨ä¸“ä¸šã€ç®€æ˜çš„æ–¹å¼å›ç­”ã€‚"""

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    logger.info(f"Request llm server payload: {payload}")
    response = requests.post(SystemConfig.ollama_url, json=payload)
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"è¯·æ±‚å¤±è´¥: {response.status_code}\n{response.text}"

def is_punctuation(char):

    if not isinstance(char, str) or len(char) != 1:
        # logger.info(f"ä¼ å…¥çš„ charï¼š{repr(char)}ï¼Œé•¿åº¦ï¼š{len(char)}   not isinstance(char, str) or len(char) != 1: {False}")
        return False

    # æ”¾è¡Œè‹±æ–‡æ ‡ç‚¹ç¬¦å·
    if char in string.punctuation and ord(char) < 128:
        # logger.info(f"ä¼ å…¥çš„ charï¼š{repr(char)}ï¼Œé•¿åº¦ï¼š{len(char)}   char in string.punctuation and ord(char) < 128: {False}")
        return False

    # æ‹¬å·ä¸å¼•å·
    if char in ["ï¼ˆ", "ï¼‰", "(", ")", "{", "}", "[", "]", "'", "\"", "â€˜", "â€œ"]:
        return False

    # åªç®—ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
    # logger.info(f"ä¼ å…¥çš„ charï¼š{repr(char)}ï¼Œé•¿åº¦ï¼š{len(char)}   {unicodedata.category(char).startswith('P')}")
    return unicodedata.category(char).startswith('P')

# === æµå¼è¯·æ±‚ Ollamaï¼ˆæ”¯æŒé€æ®µè¿”å›ï¼‰===
def ask_with_context_stream_ollama_mistral(user_question, collection_name, model="mistral:7b-instruct", top_k=SystemConfig.default_top_k):
    results = retrieve_from_qdrant(user_question, collection_name, top_k=top_k)
    logger.info(f"qdrant results: {results}")

    if not results:
        context = "ï¼ˆæœªèƒ½ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼‰"
    else:
        context = "\n".join([
            f"ã€{r.get('section', r.get('source', 'æœªæ³¨æ˜æ¥æº'))}ã€‘\n{r.get('content', r.get('text', str(r)))}"
            for r in results
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ç¨‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯èµ„æ–™å’Œç”¨æˆ·é—®é¢˜ï¼Œç”¨**ä¸­æ–‡**å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ä¸è¦æ±‚ï¼š

    1. å›ç­”ä¸­å¿…é¡»å®Œå…¨ä½¿ç”¨ä¸­æ–‡ï¼Œ**ç¦æ­¢å‡ºç°ä»»ä½•è‹±æ–‡å•è¯æˆ–æœ¯è¯­**ï¼›
        - å³ä½¿è‹±æ–‡æœ¯è¯­ï¼ˆå¦‚ depthã€spacingã€hole ç­‰ï¼‰å‡ºç°åœ¨èƒŒæ™¯èµ„æ–™ä¸­ï¼Œä¹Ÿå¿…é¡»ç¿»è¯‘ä¸ºä¸­æ–‡ï¼›
        - **ç‰¹åˆ«å¼ºè°ƒï¼šä¸¥ç¦å‡ºç°â€œdepthâ€ã€â€œspacingâ€ã€â€œholeâ€ç­‰è¯ï¼Œå³ä½¿æ¨¡å‹è®¤ä¸ºæ›´ä¸“ä¸šä¹Ÿä¸èƒ½ä½¿ç”¨ï¼Œå¿…é¡»ç¿»è¯‘ä¸ºâ€œæ·±åº¦â€ã€â€œé—´è·â€ã€â€œå­”å·â€ç­‰ã€‚**
    2. æ¯å¥è¯é•¿åº¦**ä¸èƒ½è¶…è¿‡10ä¸ªå­—ç¬¦**ï¼ŒåŒ…æ‹¬æ‰€æœ‰**æ•°å­—ã€å•ä½ã€æ ‡ç‚¹ã€æ‹¬å·ã€å¼•å·**ï¼›
        - å¦‚æœä¸€å¥è¯è¶…è¿‡10ä¸ªå­—ç¬¦ï¼Œå¿…é¡»æ‹†åˆ†ä¸ºå¤šå¥ï¼›
        - æ‰€æœ‰å¥å­å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ï¼›
        - ä¸­é—´å¥å­åº”ä½¿ç”¨é€—å·ã€é¡¿å·ç»“å°¾ï¼Œä»…æœ€åä¸€å¥ä½¿ç”¨å¥å·ï¼›
    3. è‹¥æœ‰è¾ƒé•¿çš„è¡¨è¾¾ï¼Œè¯·**åˆ†ä¸ºå¤šå¥çŸ­å¥**é€æ¡è¾“å‡ºï¼›
    4. ä¸éœ€è¦è§£é‡Šæœ¯è¯­ã€ä¸è¦æ·»åŠ æ³¨é‡Šï¼›
    5. è¯­è¨€é£æ ¼åº”ç®€æ´ã€è‡ªç„¶ã€å£è¯­åŒ–ï¼Œé€‚åˆæœ—è¯»ä¸è¯­éŸ³æ’­æ”¾ï¼›
    6. ä¸å¾—ä½¿ç”¨ markdown æˆ–ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚ `>`, `-`, `*` ç­‰ï¼‰ï¼›
    7. ä½ å¯ä»¥æ•°ä¸€æ•°æ¯å¥å­—ç¬¦æ˜¯å¦è¶…è¿‡10ä¸ªï¼Œç¡®ä¿æ–­å¥åˆç†ã€‚

    âš ï¸ æ³¨æ„ï¼šè¿™ä¸æ˜¯å»ºè®®ï¼Œæ˜¯å¿…é¡»éµå®ˆçš„ç¡¬æ€§è¦æ±‚ã€‚

    ä¸‹é¢æ˜¯ä¸€ä¸ªä¸åˆæ ¼çš„å›ç­”ç¤ºä¾‹ï¼ˆè¿‡é•¿ä¸”åŒ…å«è‹±æ–‡ï¼‰ï¼š
    N3çš„å®é™…é‚»å­”é—´è·åœ¨130ç±³å¤„ä¸º2626.32æ¯«ç±³ï¼Œdepthä¸º130mã€‚

    è¯·æ”¹ä¸ºå¦‚ä¸‹å½¢å¼ï¼ˆåˆæ ¼ï¼‰ï¼š
    å­”å·ä¸ºN3ï¼Œ  
    é‚»å­”é—´è·åœ¨130ç±³å¤„ï¼Œ  
    å®é™…å€¼æ˜¯2626.32æ¯«ç±³ã€‚

    è¯·ä¸¥æ ¼æŒ‰è¿™ç§å½¢å¼å›ç­”ï¼Œä¸å¾—åŒ…å«ä»»ä½•è‹±æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚

    èƒŒæ™¯èµ„æ–™ï¼š
    {context}

    ç”¨æˆ·æé—®ï¼š
    {user_question}

    è¯·æ ¹æ®ä¸Šè¿°è¦æ±‚ï¼Œè¾“å‡ºæ ‡å‡†ã€ä¸“ä¸šã€åˆ†å¥åˆç†çš„ä¸­æ–‡å›ç­”ã€‚
    """

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True  # å¼€å¯æµæ¨¡å¼
    }

    logger.info(f"Request llm server payload: {payload}")

    try:
        with requests.post(SystemConfig.ollama_url, json=payload, stream=True) as response:
            if response.status_code != 200:
                yield f"[é”™è¯¯] è¯·æ±‚å¤±è´¥: {response.status_code}"
                return

            response_buffer = ""
            is_ready_to_send = False
            # ä¸€è¡Œä¸€è¡Œè¯»å–å“åº”å†…å®¹
            for line in response.iter_lines():
                if line:
                    try:
                        # Ollama çš„æ¯ä¸€è¡Œæ˜¯ JSONï¼Œå¦‚ {"response": "éƒ¨åˆ†å›ç­”", "done": false}
                        data = json.loads(line.decode("utf-8"))
                        # logger.info(f"Request llm server response: {data}")
                        response = data["response"].strip()
                        if response != "" :
                            if is_punctuation(response):
                                response_buffer += response + "\n"
                                is_ready_to_send = True
                            else:
                                if is_ready_to_send:
                                    is_ready_to_send = False
                                    chunked_response = response_buffer
                                    response_buffer = response
                                    logger.info(f"llm server response: {chunked_response}")
                                    yield chunked_response
                                else:
                                    response_buffer += response
                        if data.get("done"):
                            chunked_response = response_buffer.removesuffix("\n") + "[Heil Hitler!]" + "\n"
                            logger.info(f"llm server response: {chunked_response}")
                            yield chunked_response
                            break
                    except Exception as e:
                        logger.error(f"è§£æå¤±è´¥: {line} -> {e}")
                        raise e
    except Exception as e:
        logger.error(f"[é”™è¯¯] è¿æ¥å¼‚å¸¸: {e}")
        raise e

# === æµå¼è¯·æ±‚ Ollamaï¼ˆæ”¯æŒé€æ®µè¿”å›ï¼‰===
def ask_with_context_stream_vllm_qwen(user_question, collection_name, top_k=SystemConfig.default_top_k):
    results = retrieve_from_qdrant(user_question, collection_name, top_k=top_k)
    logger.info(f"qdrant results: {results}")

    if not results:
        context = "ï¼ˆæœªèƒ½ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼‰"
    else:
        context = "\n".join([
            f"ã€{r.get('section', r.get('source', 'æœªæ³¨æ˜æ¥æº'))}ã€‘\n{r.get('content', r.get('text', str(r)))}"
            for r in results
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ç¨‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯èµ„æ–™å’Œç”¨æˆ·é—®é¢˜ï¼Œç”¨**ä¸­æ–‡**å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ä¸è¦æ±‚ï¼š

    1. å›ç­”ä¸­å¿…é¡»å®Œå…¨ä½¿ç”¨ä¸­æ–‡ï¼Œ**ç¦æ­¢å‡ºç°ä»»ä½•è‹±æ–‡å•è¯æˆ–æœ¯è¯­**ï¼›
        - å³ä½¿è‹±æ–‡æœ¯è¯­ï¼ˆå¦‚ depthã€spacingã€hole ç­‰ï¼‰å‡ºç°åœ¨èƒŒæ™¯èµ„æ–™ä¸­ï¼Œä¹Ÿå¿…é¡»ç¿»è¯‘ä¸ºä¸­æ–‡ï¼›
        - **ç‰¹åˆ«å¼ºè°ƒï¼šä¸¥ç¦å‡ºç°â€œdepthâ€ã€â€œspacingâ€ã€â€œholeâ€ç­‰è¯ï¼Œå³ä½¿æ¨¡å‹è®¤ä¸ºæ›´ä¸“ä¸šä¹Ÿä¸èƒ½ä½¿ç”¨ï¼Œå¿…é¡»ç¿»è¯‘ä¸ºâ€œæ·±åº¦â€ã€â€œé—´è·â€ã€â€œå­”å·â€ç­‰ã€‚**
    2. æ¯å¥è¯é•¿åº¦**ä¸èƒ½è¶…è¿‡10ä¸ªå­—ç¬¦**ï¼ŒåŒ…æ‹¬æ‰€æœ‰**æ•°å­—ã€å•ä½ã€æ ‡ç‚¹ã€æ‹¬å·ã€å¼•å·**ï¼›
        - å¦‚æœä¸€å¥è¯è¶…è¿‡10ä¸ªå­—ç¬¦ï¼Œå¿…é¡»æ‹†åˆ†ä¸ºå¤šå¥ï¼›
        - æ‰€æœ‰å¥å­å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ï¼›
        - ä¸­é—´å¥å­åº”ä½¿ç”¨é€—å·ã€é¡¿å·ç»“å°¾ï¼Œä»…æœ€åä¸€å¥ä½¿ç”¨å¥å·ï¼›
    3. è‹¥æœ‰è¾ƒé•¿çš„è¡¨è¾¾ï¼Œè¯·**åˆ†ä¸ºå¤šå¥çŸ­å¥**é€æ¡è¾“å‡ºï¼›
    4. ä¸éœ€è¦è§£é‡Šæœ¯è¯­ã€ä¸è¦æ·»åŠ æ³¨é‡Šï¼›
    5. è¯­è¨€é£æ ¼åº”ç®€æ´ã€è‡ªç„¶ã€å£è¯­åŒ–ï¼Œé€‚åˆæœ—è¯»ä¸è¯­éŸ³æ’­æ”¾ï¼›
    6. ä¸å¾—ä½¿ç”¨ markdown æˆ–ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚ `>`, `-`, `*` ç­‰ï¼‰ï¼›
    7. ä½ å¯ä»¥æ•°ä¸€æ•°æ¯å¥å­—ç¬¦æ˜¯å¦è¶…è¿‡10ä¸ªï¼Œç¡®ä¿æ–­å¥åˆç†ã€‚

    âš ï¸ æ³¨æ„ï¼šè¿™ä¸æ˜¯å»ºè®®ï¼Œæ˜¯å¿…é¡»éµå®ˆçš„ç¡¬æ€§è¦æ±‚ã€‚

    ä¸‹é¢æ˜¯ä¸€ä¸ªä¸åˆæ ¼çš„å›ç­”ç¤ºä¾‹ï¼ˆè¿‡é•¿ä¸”åŒ…å«è‹±æ–‡ï¼‰ï¼š
    N3çš„å®é™…é‚»å­”é—´è·åœ¨130ç±³å¤„ä¸º2626.32æ¯«ç±³ï¼Œdepthä¸º130mã€‚

    è¯·æ”¹ä¸ºå¦‚ä¸‹å½¢å¼ï¼ˆåˆæ ¼ï¼‰ï¼š
    å­”å·ä¸ºN3ï¼Œ  
    é‚»å­”é—´è·åœ¨130ç±³å¤„ï¼Œ  
    å®é™…å€¼æ˜¯2626.32æ¯«ç±³ã€‚

    è¯·ä¸¥æ ¼æŒ‰è¿™ç§å½¢å¼å›ç­”ï¼Œä¸å¾—åŒ…å«ä»»ä½•è‹±æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚

    èƒŒæ™¯èµ„æ–™ï¼š
    {context}

    ç”¨æˆ·æé—®ï¼š
    {user_question}

    è¯·æ ¹æ®ä¸Šè¿°è¦æ±‚ï¼Œè¾“å‡ºæ ‡å‡†ã€ä¸“ä¸šã€åˆ†å¥åˆç†çš„ä¸­æ–‡å›ç­”ã€‚
    """

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True  # å¼€å¯æµæ¨¡å¼
    }

    logger.info(f"Request llm server payload: {payload}")

    try:
        with requests.post(SystemConfig.ollama_url, json=payload, stream=True) as response:
            if response.status_code != 200:
                yield f"[é”™è¯¯] è¯·æ±‚å¤±è´¥: {response.status_code}"
                return

            response_buffer = ""
            is_ready_to_send = False
            # ä¸€è¡Œä¸€è¡Œè¯»å–å“åº”å†…å®¹
            for line in response.iter_lines():
                if line:
                    try:
                        # Ollama çš„æ¯ä¸€è¡Œæ˜¯ JSONï¼Œå¦‚ {"response": "éƒ¨åˆ†å›ç­”", "done": false}
                        data = json.loads(line.decode("utf-8"))
                        # logger.info(f"Request llm server response: {data}")
                        response = data["response"].strip()
                        if response != "" :
                            if is_punctuation(response):
                                response_buffer += response + "\n"
                                is_ready_to_send = True
                            else:
                                if is_ready_to_send:
                                    is_ready_to_send = False
                                    chunked_response = response_buffer
                                    response_buffer = response
                                    logger.info(f"llm server response: {chunked_response}")
                                    yield chunked_response
                                else:
                                    response_buffer += response
                        if data.get("done"):
                            chunked_response = response_buffer.removesuffix("\n") + "[Heil Hitler!]" + "\n"
                            logger.info(f"llm server response: {chunked_response}")
                            yield chunked_response
                            break
                    except Exception as e:
                        logger.error(f"è§£æå¤±è´¥: {line} -> {e}")
                        raise e
    except Exception as e:
        logger.error(f"[é”™è¯¯] è¿æ¥å¼‚å¸¸: {e}")
        raise e


# === API è·¯ç”± ===
@app.post("/ask")
def rag_qa(req: QuestionRequest):
    request_id = str(uuid.uuid4())
    logger.info(f"[ASK æ¥æ”¶] {request_id}  é—®é¢˜: {req}")

    collection = resolve_collection(req.question)
    # default use qwen llm
    return StreamingResponse(
        ask_with_context_stream_vllm_qwen(
            req.question,
            collection,
            top_k=req.top_k),
        media_type="text/plain",
        status_code=200,
        headers={"X-Accel-Buffering": "no"}
    )




if __name__ == "__main__":
    import uvicorn

    if SystemConfig.is_dev_mode:
        uvicorn.run(
            "main:app",
            host="0.0.0.0",
            port=8000,
            reload=SystemConfig.is_dev_mode,
            log_config="log_config.yml"
        )
    else:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            reload=SystemConfig.is_dev_mode,
            log_config="log_config.yml"
        )